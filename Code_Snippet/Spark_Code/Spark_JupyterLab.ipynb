{"cells": [{"cell_type": "code", "execution_count": 44, "id": "0e95595f-d068-4547-8140-f946d00aae8a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 16:25:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when , lit\n\n# Create a Spark session\nspark = SparkSession.builder \\\n    .appName(\"OuterJoinExample\") \\\n    .getOrCreate()\n\ns3_path1 = 'gs://gstock_data2/output_google.csv'\ns3_path2 = 'gs://inputdatastock/GOOG.csv'"}, {"cell_type": "code", "execution_count": 45, "id": "598178c8-53b3-4d31-b82b-187a8fe7fed4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df1 = spark.read.csv(s3_path1, header=True, inferSchema=True)\ndf2 = spark.read.csv(s3_path2, header=True, inferSchema=True)"}, {"cell_type": "code", "execution_count": 46, "id": "6e596f7d-f7f6-42a6-88bf-5e2776bd3d23", "metadata": {}, "outputs": [], "source": "df1 = df1.withColumn(\"stock_name\", lit(\"Google\"))\ndf2 = df2.withColumn(\"stock_name\", lit(\"Google\"))"}, {"cell_type": "code", "execution_count": 47, "id": "ef28030b-6140-4243-8134-161a0b1d52c8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+------+------+--------+------+-----------+-------+----------+\n|            Date|  Open|  High|     Low| Close| Adj. Close| Volume|stock_name|\n+----------------+------+------+--------+------+-----------+-------+----------+\n|13-12-2010 00:00|597.12| 603.0|  594.09|594.62|298.2306093|4792200|    Google|\n|14-12-2010 00:00|597.09|598.29|  592.48|594.91|298.3760583|3283300|    Google|\n|15-12-2010 00:00| 594.2|596.45|589.1501| 590.3| 296.063921|4331000|    Google|\n|16-12-2010 00:00|592.85|593.77|  588.07|591.71| 296.771104|3190600|    Google|\n|17-12-2010 00:00| 591.0|592.56|  587.67| 590.8|296.3146951|6168000|    Google|\n+----------------+------+------+--------+------+-----------+-------+----------+\nonly showing top 5 rows\n\n"}], "source": "df1.show(5)"}, {"cell_type": "code", "execution_count": 48, "id": "2d3724d0-7b31-4e94-9062-7a4dbef6baff", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+---------+---------+---------+---------+---------+--------+----------+\n|               Date|     Open|     High|      Low|    Close|Adj Close|  Volume|stock_name|\n+-------------------+---------+---------+---------+---------+---------+--------+----------+\n|2018-03-28 00:00:00|49.900002|51.211498|49.032001|50.228001|50.228001|67386000|    Google|\n|2018-03-29 00:00:00|50.581501|52.150002|   50.145|  51.5895|  51.5895|54536000|    Google|\n|2018-04-02 00:00:00|51.140999|51.740002|49.518501|50.323502|50.323502|53608000|    Google|\n|2018-04-03 00:00:00|50.695499|  51.0495|49.703499|50.670502|50.670502|45502000|    Google|\n|2018-04-04 00:00:00|49.670502|51.435902|49.650002|   51.257|   51.257|49694000|    Google|\n+-------------------+---------+---------+---------+---------+---------+--------+----------+\nonly showing top 5 rows\n\n"}], "source": "df2.show(5)"}, {"cell_type": "code", "execution_count": 49, "id": "293980a7-4004-412c-b4d6-a4af7077f90e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+------+--------+--------+------+-----------+-------+----------+\n|            Date|  Open|    High|     Low| Close| Adj. Close| Volume|stock_name|\n+----------------+------+--------+--------+------+-----------+-------+----------+\n|13-12-2010 00:00|597.12|   603.0|  594.09|594.62|298.2306093|4792200|    Google|\n|14-12-2010 00:00|597.09|  598.29|  592.48|594.91|298.3760583|3283300|    Google|\n|15-12-2010 00:00| 594.2|  596.45|589.1501| 590.3| 296.063921|4331000|    Google|\n|16-12-2010 00:00|592.85|  593.77|  588.07|591.71| 296.771104|3190600|    Google|\n|17-12-2010 00:00| 591.0|  592.56|  587.67| 590.8|296.3146951|6168000|    Google|\n|20-12-2010 00:00|594.65|597.8799|588.6565|595.06|298.4512905|3942600|    Google|\n|21-12-2010 00:00|598.57|604.7199| 597.615|603.07|302.4686919|3755200|    Google|\n|22-12-2010 00:00| 604.0|   607.0|  603.28|605.49|303.6824386|2412500|    Google|\n|23-12-2010 00:00|605.34|   606.0|  602.03|604.23|303.0504878|2219300|    Google|\n|27-12-2010 00:00|602.74|  603.78|   599.5|602.38|302.1226236|2413700|    Google|\n|28-12-2010 00:00|602.05|  603.87|  598.01|598.92|300.3872667|2127400|    Google|\n|29-12-2010 00:00| 602.0|  602.41|  598.92| 601.0| 301.430487|2036300|    Google|\n|30-12-2010 00:00| 598.0| 601.334|597.3901|598.86|300.3571738|1977000|    Google|\n|31-12-2010 00:00|596.74|  598.42|  592.03|593.97| 297.904603|3075500|    Google|\n|03-01-2011 00:00|596.48|  605.59|  596.48|604.35|303.1106736|4725600|    Google|\n|04-01-2011 00:00|605.62|  606.18|  600.12|602.12|301.9922211|3645300|    Google|\n|05-01-2011 00:00|600.07|  610.33|  600.05|609.07|305.4779813|5059500|    Google|\n|06-01-2011 00:00|610.68|  618.43|  610.05| 613.5|307.6998399|4111400|    Google|\n|07-01-2011 00:00|615.91|  618.25|  610.13|616.44|309.1743917|4198100|    Google|\n|10-01-2011 00:00| 614.8|  615.39|  608.56|614.21|308.0559392|3155200|    Google|\n+----------------+------+--------+--------+------+-----------+-------+----------+\nonly showing top 20 rows\n\n"}], "source": "\n# Define the common column to join on\ncommon_column = \"Date\"\n\n# Perform the outer join operation\ncombined_df = df1.union(df2)\n\n# Show the resulting DataFrame\ncombined_df.show()"}, {"cell_type": "code", "execution_count": 50, "id": "fc45e8d4-152e-4f28-b147-1fe8f274fb63", "metadata": {}, "outputs": [{"data": {"text/plain": "2772"}, "execution_count": 50, "metadata": {}, "output_type": "execute_result"}], "source": "num_rows = combined_df.count()\n\n# Get the number of columns\nnum_rows"}, {"cell_type": "code", "execution_count": null, "id": "3aaafd6d-d225-4640-b311-510c08e9cd34", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"ExportCombinedData\").getOrCreate()\n\n# Define the output path in GCS\noutput_path = \"gs://output_bucket_datasets/combined_google_df.csv\"\n\n# Write the combined DataFrame to a CSV file in GCS\ncombined_df.write.csv(output_path, header=True, mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": 51, "id": "7c6cdb45-4cde-4dd1-87b8-bf347040dbfc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 16:26:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+----------------+------------------+------------------+------------------+----------------+------------------+--------------------+----------+\n|summary|            Date|              Open|              High|               Low|           Close|        Adj. Close|              Volume|stock_name|\n+-------+----------------+------------------+------------------+------------------+----------------+------------------+--------------------+----------+\n|  count|            2772|              2772|              2772|              2772|            2772|              2772|                2772|      2772|\n|   mean|            null| 519.1502297813857| 523.2266671702731|  514.643472629511|519.043394873376|409.56053325389615|1.2839723719696969E7|      null|\n| stddev|            null|349.08919269399416|351.51532059792305|346.24123453151583|348.936012657324|307.80667617874695|1.5642162295457924E7|      null|\n|    min|01-02-2011 00:00|            48.695|         50.176998|         48.505501|       48.811001|         48.811001|              521141|    Google|\n|    max|31-12-2015 00:00|            1226.8|           1228.88|            1218.6|         1220.17|           1187.56|           124140000|    Google|\n+-------+----------------+------------------+------------------+------------------+----------------+------------------+--------------------+----------+\n\n"}], "source": "from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"Google_Describe\").getOrCreate()\n\n\nGoogle = combined_df\n\n# Use the describe method to compute basic statistics\ndescription = Google.describe()\n\n# Show the result\ndescription.show()\n\n# Stop the Spark session\nspark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "3c8676cf-de1a-4198-ad58-579d5b4128b7", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 46, "id": "a8bf7a80-5076-4251-a777-fdfd58c4cc96", "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'gstock_data2' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_10114/2361142903.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgstock_data2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'gstock_data2' is not defined"]}], "source": "gstock_data2"}, {"cell_type": "code", "execution_count": 109, "id": "e12ce4c0-310a-42ae-8d22-b91d3957463c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 20:13:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Tweets:\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 174:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+---------------+----------+--------------------+-----------+-----------+--------+\n|          tweet_id|         writer| post_date|                body|comment_num|retweet_num|like_num|\n+------------------+---------------+----------+--------------------+-----------+-----------+--------+\n|550441509175443456|VisualStockRSRC|1420070457|lx21 made $10,008...|          0|          0|       1|\n|550441672312512512|    KeralaGuy77|1420070496|Insanity of today...|          0|          0|       0|\n|550441732014223360|    DozenStocks|1420070510|S&P100 #Stocks Pe...|          0|          0|       0|\n|550442977802207232|   ShowDreamCar|1420070807|$GM $TSLA: Volksw...|          0|          0|       1|\n|550443807834402816|   i_Know_First|1420071005|Swing Trading: Up...|          0|          0|       1|\n+------------------+---------------+----------+--------------------+-----------+-----------+--------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_unixtime\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom pyspark.sql.functions import col\nimport nltk\n\n\n# Replace 'your_file_path.csv' with the actual path to your CSV file\nCOMPANY_TWEETS = \"gs://output_samar/Company_Tweet.csv\"\nTWEETS= \"gs://output_samar/Tweet.csv\"\nOUTPUT = \"gs://output_samar/output\"\n# meta_Tweets = \"gs://input_samar/meta_tweets\"\n# google_Tweets = \"gs://input_samar/google_tweets\"\n# tesla_Tweets = \"gs://input_samar/tesla_tweets\"\n\nspark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n\n# Read the CSV file into a DataFrame\nCompany_Tweet = spark.read.csv(COMPANY_TWEETS, header=True, inferSchema=True)\nTweet = spark.read.csv(TWEETS, header=True, inferSchema=True)\n\nprint(\"Tweets:\")\nTweet.show(5)"}, {"cell_type": "code", "execution_count": 110, "id": "5600617f-d820-4bf0-973f-53f22ec5cfb4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 179:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+---------------+----------+--------------------+-----------+-----------+--------+-------------+-------------------+\n|          tweet_id|         writer| post_date|                body|comment_num|retweet_num|like_num|ticker_symbol|   timestamp_column|\n+------------------+---------------+----------+--------------------+-----------+-----------+--------+-------------+-------------------+\n|550447574285418497|      btcgemini|1420071903|We searched throu...|          0|          0|       0|         AAPL|2015-01-01 00:25:03|\n|550450683309072384|     10Xtrading|1420072644|Clay Trader Apple...|          0|          0|       0|         AAPL|2015-01-01 00:37:24|\n|550455355134578688|  BoilerWarrior|1420073758|http://StockAviat...|          0|          0|       0|         MSFT|2015-01-01 00:55:58|\n|550460180631011328|Maximum_Pain_cm|1420074908|$AMZN OI for matu...|          0|          0|       0|         AMZN|2015-01-01 01:15:08|\n|550461555423584257|     t_nathan95|1420075236|Prediction: $TWTR...|          0|          0|       1|         GOOG|2015-01-01 01:20:36|\n+------------------+---------------+----------+--------------------+-----------+-----------+--------+-------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Merge the two DataFrames on the 'tweet_id' column\nmerged_df = Tweet.join(Company_Tweet, on='tweet_id')\n\n\n# Converting seconds to date time format\nmerged_df = merged_df.withColumn(\"timestamp_column\", from_unixtime(merged_df[\"post_date\"]).cast(\"timestamp\"))\nmerged_df.show(5)"}, {"cell_type": "code", "execution_count": 112, "id": "4c12c890-81de-42c9-8318-f07fc9c96a42", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 189:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+---------------+----------+--------------------+-----------+-----------+--------+-------------+-------------------+----------+\n|          tweet_id|         writer| post_date|                body|comment_num|retweet_num|like_num|ticker_symbol|   timestamp_column|      date|\n+------------------+---------------+----------+--------------------+-----------+-----------+--------+-------------+-------------------+----------+\n|550447574285418497|      btcgemini|1420071903|We searched throu...|          0|          0|       0|         AAPL|2015-01-01 00:25:03|2015-01-01|\n|550450683309072384|     10Xtrading|1420072644|Clay Trader Apple...|          0|          0|       0|         AAPL|2015-01-01 00:37:24|2015-01-01|\n|550455355134578688|  BoilerWarrior|1420073758|http://StockAviat...|          0|          0|       0|         MSFT|2015-01-01 00:55:58|2015-01-01|\n|550460180631011328|Maximum_Pain_cm|1420074908|$AMZN OI for matu...|          0|          0|       0|         AMZN|2015-01-01 01:15:08|2015-01-01|\n|550461555423584257|     t_nathan95|1420075236|Prediction: $TWTR...|          0|          0|       1|         GOOG|2015-01-01 01:20:36|2015-01-01|\n+------------------+---------------+----------+--------------------+-----------+-----------+--------+-------------+-------------------+----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col, date_format\n\nmerged_df = merged_df.withColumn('date', date_format(col('timestamp_column'), 'yyyy-MM-dd'))\nmerged_df.show(5)"}, {"cell_type": "code", "execution_count": null, "id": "73f5a432-42ec-4572-b8a5-6b310531ea37", "metadata": {}, "outputs": [], "source": "#TWITTER SENTIMENTS PROCESSING"}, {"cell_type": "code", "execution_count": 13, "id": "2b176bd3-21b6-4927-b0d8-da353a82acc2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"}, {"data": {"text/plain": "True"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "# Download the VADER lexicon for sentiment analysis (run this only once)\nnltk.download('vader_lexicon')"}, {"cell_type": "code", "execution_count": 85, "id": "87015b84-4ed4-49e1-a05c-caae238632b6", "metadata": {}, "outputs": [], "source": "# Initialize the VADER sentiment intensity analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Define a UDF (User Defined Function) to apply sentiment analysis to the 'body' column\ndef analyze_sentiment(text):\n  sentiment_score = sia.polarity_scores(text)['compound']\n  if sentiment_score >= 0.05:\n      return 'positive'\n  elif sentiment_score <= -0.05:\n      return 'negative'\n  else:\n      return 'neutral'\n\n# Register the UDF\nsentiment_analysis_udf = udf(analyze_sentiment, StringType())\n"}, {"cell_type": "code", "execution_count": 123, "id": "a2ef384f-fcbd-45a9-b542-52c35ce2f81c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 269:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------+----------+---------+\n|ticker_symbol|      date|sentiment|\n+-------------+----------+---------+\n|         AAPL|2015-01-01|  neutral|\n|         AAPL|2015-01-01| negative|\n|         AAPL|2015-01-01| positive|\n|         AAPL|2015-01-01|  neutral|\n|         AMZN|2015-01-01| negative|\n+-------------+----------+---------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Apply sentiment analysis to the 'body' column and create a new column 'sentiment'\nmerged_df = merged_df.withColumn(\"sentiment\", sentiment_analysis_udf(merged_df[\"body\"]))\nmerged_df_new = merged_df.select('ticker_symbol','date','sentiment')\nmerged_df_new.show(5)\n"}, {"cell_type": "code", "execution_count": 124, "id": "37822609-2402-4437-8100-c0ed022a6555", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "merged_df_new.write.csv(OUTPUT, header=True, mode=\"overwrite\")\n"}, {"cell_type": "code", "execution_count": null, "id": "99ec2dd8-598c-45c5-8cda-02c764e0ac64", "metadata": {}, "outputs": [], "source": "#END"}, {"cell_type": "code", "execution_count": 52, "id": "1c8cec95-51a3-408e-a841-3146bbda6541", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 16:28:00 INFO SparkEnv: Registering MapOutputTracker\n23/12/19 16:28:00 INFO SparkEnv: Registering BlockManagerMaster\n23/12/19 16:28:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n23/12/19 16:28:00 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when , lit\n\n# Create a Spark session\nspark = SparkSession.builder \\\n    .appName(\"OuterJoinExample\") \\\n    .getOrCreate()\n\ns3_path1 = 'gs://inputdataapple/AAPL.csv'\ns3_path2 = 'gs://inputdataapple/output_apple.csv'"}, {"cell_type": "code", "execution_count": 53, "id": "ff515868-36f9-43a1-8103-f12e6aca2b33", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df1 = spark.read.csv(s3_path1, header=True, inferSchema=True)\ndf2 = spark.read.csv(s3_path2, header=True, inferSchema=True)"}, {"cell_type": "code", "execution_count": 54, "id": "5fefaf43-cbeb-4805-8e60-268888af1573", "metadata": {}, "outputs": [], "source": "df1 = df1.withColumn(\"stock_name\", lit(\"Apple\"))\ndf2 = df2.withColumn(\"stock_name\", lit(\"Apple\"))"}, {"cell_type": "code", "execution_count": 55, "id": "ee9f63d4-4f33-4b96-b2d0-7a04b740a35d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|               Date|     Open|     High|      Low|    Close|Adj Close|   Volume|stock_name|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|2018-03-28 00:00:00|  41.8125|42.505001|41.297501|41.619999|39.516926|166674000|     Apple|\n|2018-03-29 00:00:00|41.952499|  42.9375|41.724998|   41.945|  39.8255|153594000|     Apple|\n|2018-04-02 00:00:00|    41.66|42.235001|  41.1175|41.669998|  39.5644|150347200|     Apple|\n|2018-04-03 00:00:00|    41.91|  42.1875|41.220001|  42.0975|39.970299|121112000|     Apple|\n|2018-04-04 00:00:00|41.220001|43.002499|41.192501|  42.9025|40.734608|138422000|     Apple|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\nonly showing top 5 rows\n\n"}], "source": "df1.show(5)"}, {"cell_type": "code", "execution_count": 56, "id": "f077ffa9-7a49-4304-b792-f1a1183a0077", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+------+------+------+------+-----------+--------+----------+\n|            Date|  Open|  High|   Low| Close| Adj. Close|  Volume|stock_name|\n+----------------+------+------+------+------+-----------+--------+----------+\n|13-12-2010 00:00|324.37|325.06| 321.0|321.67|41.33907832|15707700|     Apple|\n|14-12-2010 00:00|321.73|322.54| 319.0|320.29|41.16172909|12536000|     Apple|\n|15-12-2010 00:00| 320.0| 323.0|319.19|320.36|41.17072507|14904000|     Apple|\n|16-12-2010 00:00|321.09|322.61| 320.1|321.25|41.28510247|11501100|     Apple|\n|17-12-2010 00:00|321.63|321.79|320.23|320.61|41.20285355|13818900|     Apple|\n+----------------+------+------+------+------+-----------+--------+----------+\nonly showing top 5 rows\n\n"}], "source": "df2.show(5)"}, {"cell_type": "code", "execution_count": 57, "id": "cf296feb-c4e3-4a6f-bcee-475901b65521", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|               Date|     Open|     High|      Low|    Close|Adj Close|   Volume|stock_name|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|2018-03-28 00:00:00|  41.8125|42.505001|41.297501|41.619999|39.516926|166674000|     Apple|\n|2018-03-29 00:00:00|41.952499|  42.9375|41.724998|   41.945|  39.8255|153594000|     Apple|\n|2018-04-02 00:00:00|    41.66|42.235001|  41.1175|41.669998|  39.5644|150347200|     Apple|\n|2018-04-03 00:00:00|    41.91|  42.1875|41.220001|  42.0975|39.970299|121112000|     Apple|\n|2018-04-04 00:00:00|41.220001|43.002499|41.192501|  42.9025|40.734608|138422000|     Apple|\n|2018-04-05 00:00:00|   43.145|43.557499|    43.02|43.200001|41.017086|107732800|     Apple|\n|2018-04-06 00:00:00|  42.7425|43.119999|42.049999|42.095001|39.967922|140021200|     Apple|\n|2018-04-09 00:00:00|42.470001|43.272499|42.462502|42.512501|40.364326|116070800|     Apple|\n|2018-04-10 00:00:00|    43.25|     43.5|  42.8825|  43.3125|41.123905|113634400|     Apple|\n|2018-04-11 00:00:00|43.057499|    43.48|42.924999|43.110001|40.931625| 89726400|     Apple|\n|2018-04-12 00:00:00|43.352501|    43.75|43.259998|   43.535|41.335155| 91557200|     Apple|\n|2018-04-13 00:00:00|   43.695|43.959999|43.462502|43.682499|41.475212|100497200|     Apple|\n|2018-04-16 00:00:00|  43.7575|44.047501|  43.7075|43.955002|41.733936| 86313600|     Apple|\n|2018-04-17 00:00:00|44.122501|44.735001|44.102501|44.560001|42.308376|106421600|     Apple|\n|2018-04-18 00:00:00|44.452499|44.705002|44.220001|44.459999|42.213402| 83018000|     Apple|\n|2018-04-19 00:00:00|43.439999|  43.8475|43.165001|43.200001|41.017086|139235200|     Apple|\n|2018-04-20 00:00:00|42.650002|   42.805|41.357498|    41.43|39.336525|261964400|     Apple|\n|2018-04-23 00:00:00|  41.7075|    41.73|41.022499|41.310001|39.222591|146062000|     Apple|\n|2018-04-24 00:00:00|  41.4175|  41.5825|   40.305|40.735001|38.676643|134768000|     Apple|\n|2018-04-25 00:00:00|40.654999|   41.355|40.602501|40.912498|38.845165|113528400|     Apple|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\nonly showing top 20 rows\n\n"}], "source": "common_column = \"Date\"\n\ncombined_df = df1.union(df2)\n\ncombined_df.show()"}, {"cell_type": "code", "execution_count": 7, "id": "6ff1a34e-9fbe-434f-ad92-51e8c661ded0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 15:48:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}], "source": "from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"ExportCombinedData\").getOrCreate()\n\n# Define the output path in GCS\noutput_path = \"gs://output_bucket_datasets/combined_apple_df.csv\"\n\n# Write the combined DataFrame to a CSV file in GCS\ncombined_df.write.csv(output_path, header=True, mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": 8, "id": "64c63b37-b47e-4ae4-b185-369241fd7885", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "2780"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "num_rows = combined_df.count()\n\n# Get the number of columns\nnum_rows"}, {"cell_type": "code", "execution_count": 58, "id": "32ba09be-2e58-4ca7-b4a1-dbca3bbd1e70", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 16:28:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+----------------+------------------+------------------+------------------+-----------------+------------------+--------------------+----------+\n|summary|            Date|              Open|              High|               Low|            Close|         Adj Close|              Volume|stock_name|\n+-------+----------------+------------------+------------------+------------------+-----------------+------------------+--------------------+----------+\n|  count|            2780|              2780|              2780|              2780|             2780|              2780|                2780|      2780|\n|   mean|            null|223.05853067482013|225.10339854928074|220.82915491043173| 223.005911322302| 90.22356896482022| 6.091945360395683E7|      null|\n| stddev|            null|184.85994316902384| 186.4493127897135|182.92308183484684|184.6940968548492|37.792829955186804|5.8051722603062585E7|      null|\n|    min|01-02-2011 00:00|         35.994999|             36.43|              35.5|        35.547501|          34.11887|             5624800|     Apple|\n|    max|31-12-2015 00:00|            702.41|            705.07|            699.57|            702.1|            181.72|           426510000|     Apple|\n+-------+----------------+------------------+------------------+------------------+-----------------+------------------+--------------------+----------+\n\n"}], "source": "from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"Apple_Describe\").getOrCreate()\n\n\nApple = combined_df\n\n# Use the describe method to compute basic statistics\ndescription = Apple.describe()\n\n# Show the result\ndescription.show()\n\n# Stop the Spark session\nspark.stop()"}, {"cell_type": "code", "execution_count": 36, "id": "9dfc13fa-088b-4cb3-8527-9e387c7c4023", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when , lit\n\n# Create a Spark session\nspark = SparkSession.builder \\\n    .appName(\"OuterJoinExample\") \\\n    .getOrCreate()\n\ns3_path1 = 'gs://inputdatatesla/TSLA.csv'\ns3_path2 = 'gs://inputdatatesla/output_tesla.csv'"}, {"cell_type": "code", "execution_count": 37, "id": "104a904a-23d4-4b97-8112-cc83813508db", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df1 = spark.read.csv(s3_path1, header=True, inferSchema=True)\ndf2 = spark.read.csv(s3_path2, header=True, inferSchema=True)"}, {"cell_type": "code", "execution_count": 38, "id": "a25ae272-281f-40f3-b173-272ac6c877ac", "metadata": {}, "outputs": [], "source": "df1 = df1.withColumn(\"stock_name\", lit(\"Tesla\"))\ndf2 = df2.withColumn(\"stock_name\", lit(\"Tesla\"))"}, {"cell_type": "code", "execution_count": 39, "id": "18d25625-3d9b-430d-84bd-ee41e07329f2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|               Date|     Open|     High|      Low|    Close|Adj Close|   Volume|stock_name|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|2018-03-28 00:00:00|17.638666|17.912001|16.806667|17.185333|17.185333|315021000|     Tesla|\n|2018-03-29 00:00:00|17.099333|18.063999|16.547333|17.742001|17.742001|227560500|     Tesla|\n|2018-04-02 00:00:00|   17.084|17.355333|   16.306|16.832001|16.832001|241710000|     Tesla|\n|2018-04-03 00:00:00|17.988001|18.223333|   16.966|17.835333|17.835333|282666000|     Tesla|\n|2018-04-04 00:00:00|16.851999|19.224667|16.799999|19.129333|19.129333|298450500|     Tesla|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\nonly showing top 5 rows\n\n"}], "source": "df1.show(5)"}, {"cell_type": "code", "execution_count": 40, "id": "bcc38188-5e62-40ab-bacd-d9f09a4b3c78", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+-----+-------+-------+-----+----------+-------+----------+\n|            Date| Open|   High|    Low|Close|Adj. Close| Volume|stock_name|\n+----------------+-----+-------+-------+-----+----------+-------+----------+\n|13-12-2010 00:00|31.64|  31.77|   30.4|30.55|     30.55| 410400|     Tesla|\n|14-12-2010 00:00|30.29|30.3899| 27.761|28.53|     28.53|1765700|     Tesla|\n|15-12-2010 00:00|28.67|  29.97|  28.53| 29.6|      29.6| 742900|     Tesla|\n|16-12-2010 00:00| 30.0|  30.91|29.6501|30.81|     30.81| 790100|     Tesla|\n|17-12-2010 00:00|31.34|  31.54|  30.71|31.36|     31.36| 813000|     Tesla|\n+----------------+-----+-------+-------+-----+----------+-------+----------+\nonly showing top 5 rows\n\n"}], "source": "df2.show(5)"}, {"cell_type": "code", "execution_count": 41, "id": "19dfbd8f-6c91-4af2-9d39-1e6a0379a2ed", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|               Date|     Open|     High|      Low|    Close|Adj Close|   Volume|stock_name|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\n|2018-03-28 00:00:00|17.638666|17.912001|16.806667|17.185333|17.185333|315021000|     Tesla|\n|2018-03-29 00:00:00|17.099333|18.063999|16.547333|17.742001|17.742001|227560500|     Tesla|\n|2018-04-02 00:00:00|   17.084|17.355333|   16.306|16.832001|16.832001|241710000|     Tesla|\n|2018-04-03 00:00:00|17.988001|18.223333|   16.966|17.835333|17.835333|282666000|     Tesla|\n|2018-04-04 00:00:00|16.851999|19.224667|16.799999|19.129333|19.129333|298450500|     Tesla|\n|2018-04-05 00:00:00|19.289333|20.417334|19.213333|20.381332|20.381332|286816500|     Tesla|\n|2018-04-06 00:00:00|20.066668|20.618668|19.700001|19.953333|19.953333|202804500|     Tesla|\n|2018-04-09 00:00:00|20.024668|20.633333|19.280666|19.310667|19.310667|153747000|     Tesla|\n|2018-04-10 00:00:00|19.931334|20.473333|19.578667|20.313334|20.313334|164847000|     Tesla|\n|2018-04-11 00:00:00|20.049334|20.598667|19.977333|   20.062|   20.062|112243500|     Tesla|\n|2018-04-12 00:00:00|20.154667|20.263332|19.578667|19.605333|19.605333|114132000|     Tesla|\n|2018-04-13 00:00:00|    20.24|20.263332|   19.732|20.022667|20.022667|109908000|     Tesla|\n|2018-04-16 00:00:00|19.933332|19.977333|19.267332|   19.414|   19.414| 95077500|     Tesla|\n|2018-04-17 00:00:00|19.257999|19.478001|   18.834|19.179333|19.179333|105000000|     Tesla|\n|2018-04-18 00:00:00|19.405333|20.016001|19.210667|19.556667|19.556667| 98365500|     Tesla|\n|2018-04-19 00:00:00|19.405333|20.067333|19.236668|20.005333|20.005333| 91359000|     Tesla|\n|2018-04-20 00:00:00|19.677999|19.998667|19.316668|19.349333|19.349333| 84418500|     Tesla|\n|2018-04-23 00:00:00|19.419333|19.441334|18.822001|18.891333|18.891333| 73401000|     Tesla|\n|2018-04-24 00:00:00|     19.0|19.139334|18.563999|18.897333|18.897333| 85279500|     Tesla|\n|2018-04-25 00:00:00|     18.9|19.010668|18.483334|18.712667|18.712667| 60204000|     Tesla|\n+-------------------+---------+---------+---------+---------+---------+---------+----------+\nonly showing top 20 rows\n\n"}], "source": "common_column = \"Date\"\n\ncombined_df = df1.union(df2)\n\ncombined_df.show()"}, {"cell_type": "code", "execution_count": 20, "id": "9430bdf0-f319-4715-b34f-356a6c1595c5", "metadata": {}, "outputs": [{"data": {"text/plain": "2772"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "num_rows = combined_df.count()\n\n# Get the number of columns\nnum_rows"}, {"cell_type": "code", "execution_count": 21, "id": "2e722783-12d6-405d-acf6-8f8deba373fd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 15:56:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}], "source": "from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"ExportCombinedData\").getOrCreate()\n\n# Define the output path in GCS\noutput_path = \"gs://output_bucket_datasets/combined_tesla_df.csv\"\n\n# Write the combined DataFrame to a CSV file in GCS\ncombined_df.write.csv(output_path, header=True, mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": 42, "id": "1008bfa1-c823-411c-9854-1f0ae7fab679", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/12/19 16:21:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+----------------+------------------+------------------+------------------+------------------+------------------+-------------------+----------+\n|summary|            Date|              Open|              High|               Low|             Close|         Adj Close|             Volume|stock_name|\n+-------+----------------+------------------+------------------+------------------+------------------+------------------+-------------------+----------+\n|  count|            2772|              2772|              2772|              2772|              2772|              2772|               2772|      2772|\n|   mean|            null|144.93106920887448|147.50552565728722|142.16399961291478|144.92330989249626|144.92330989249626|5.324405609992785E7|      null|\n| stddev|            null|111.97472371250208|113.66392659010755| 110.0693070200716| 111.9228503064317| 111.9228503064317|8.874408565031748E7|      null|\n|    min|01-02-2011 00:00|         12.073333|         12.445333|         11.799333|         11.931333|         11.931333|             239600|     Tesla|\n|    max|31-12-2015 00:00|        411.470001|        414.496674|        405.666656|        409.970001|        409.970001|          914082000|     Tesla|\n+-------+----------------+------------------+------------------+------------------+------------------+------------------+-------------------+----------+\n\n"}], "source": "from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"tesla_Describe\").getOrCreate()\n\n\nTesla = combined_df\n\n# Use the describe method to compute basic statistics\ndescription = Tesla.describe()\n\n# Show the result\ndescription.show()\n\n# Stop the Spark session\nspark.stop()\n"}, {"cell_type": "code", "execution_count": 59, "id": "67103bb4-5764-48c7-b268-ac0ab3bf03be", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[Date: string, Open: double, High: double, Low: double, Close: double, Adj Close: double, Volume: int, stock_name: string]"}, "execution_count": 59, "metadata": {}, "output_type": "execute_result"}], "source": "combined_df"}, {"cell_type": "code", "execution_count": null, "id": "a80f0274-f0ad-4369-ad7f-4f7d03b1b67a", "metadata": {}, "outputs": [], "source": "#PREDICTION"}, {"cell_type": "code", "execution_count": 65, "id": "b472e1c9-1a92-46ec-b918-e9353018b7fb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+----------+-------+------------------+\n|               Date|Stock name|  Close|        prediction|\n+-------------------+----------+-------+------------------+\n|2010-12-15 00:00:00|     Apple| 320.36|320.64360400648644|\n|2010-12-21 00:00:00|     Apple|324.205|323.97392494931535|\n|2010-12-23 00:00:00|     Apple|  323.6|  323.710397915619|\n|2010-12-31 00:00:00|     Apple| 322.56| 322.4955580071912|\n|2011-01-10 00:00:00|     Apple|342.455| 341.9558463327928|\n|2011-01-14 00:00:00|     Apple| 348.48|347.99432140642165|\n|2011-01-25 00:00:00|     Apple|  341.4|340.61560061149265|\n|2011-02-02 00:00:00|     Apple| 344.32| 344.3578138376179|\n|2011-02-16 00:00:00|     Apple| 363.13| 363.1589174733379|\n|2011-02-17 00:00:00|     Apple|  358.3|358.41944226926506|\n|2011-02-18 00:00:00|     Apple| 350.56|351.33369111466067|\n|2011-02-23 00:00:00|     Apple| 342.62|342.55787598327254|\n|2011-02-25 00:00:00|     Apple| 348.16|347.85510526291745|\n|2011-03-03 00:00:00|     Apple| 359.56| 359.1665728068847|\n|2011-03-14 00:00:00|     Apple| 353.56| 353.7112783608036|\n|2011-03-23 00:00:00|     Apple| 339.19|338.82852635580537|\n|2011-04-04 00:00:00|     Apple| 341.19| 341.0931665879515|\n|2011-05-06 00:00:00|     Apple| 346.66|346.95394398147846|\n|2011-05-24 00:00:00|     Apple| 332.19| 332.4572073461659|\n|2011-05-31 00:00:00|     Apple| 347.83|347.15090411796666|\n+-------------------+----------+-------+------------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"StockPricePrediction\").getOrCreate()\n\n\ncombined_df = spark.read.csv(\"gs://inputdataall/Final_all_stock_data.csv\", header=True, inferSchema=True)\n\n\n# Feature engineering: Create a feature vector from input columns\nfeature_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\nvector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\n# Define a linear regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\", predictionCol=\"prediction\", regParam=0.01)\n\n\n# Create a pipeline\npipeline = Pipeline(stages=[vector_assembler, lr])\n\n# Split the data into training and testing sets\n(training_data, testing_data) = combined_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train the model\nmodel = pipeline.fit(training_data)\n\n# Make predictions on the testing data\npredictions = model.transform(testing_data)\n\n# Show predicted stock prices\npredictions.select(\"Date\", \"Stock name\", \"Close\", \"prediction\").show()\n\n\n\n# Stop the Spark session\nspark.stop()\n\n"}, {"cell_type": "code", "execution_count": 107, "id": "23d9da59-2969-4d64-8352-fee4502d6045", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+----------+-------+------------------+\n|               Date|Stock name|  Close|        prediction|\n+-------------------+----------+-------+------------------+\n|2010-12-15 00:00:00|     Apple| 320.36|320.64360400648644|\n|2010-12-21 00:00:00|     Apple|324.205|323.97392494931535|\n|2010-12-23 00:00:00|     Apple|  323.6|  323.710397915619|\n|2010-12-31 00:00:00|     Apple| 322.56| 322.4955580071912|\n|2011-01-10 00:00:00|     Apple|342.455| 341.9558463327928|\n|2011-01-14 00:00:00|     Apple| 348.48|347.99432140642165|\n|2011-01-25 00:00:00|     Apple|  341.4|340.61560061149265|\n|2011-02-02 00:00:00|     Apple| 344.32| 344.3578138376179|\n|2011-02-16 00:00:00|     Apple| 363.13| 363.1589174733379|\n|2011-02-17 00:00:00|     Apple|  358.3|358.41944226926506|\n|2011-02-18 00:00:00|     Apple| 350.56|351.33369111466067|\n|2011-02-23 00:00:00|     Apple| 342.62|342.55787598327254|\n|2011-02-25 00:00:00|     Apple| 348.16|347.85510526291745|\n|2011-03-03 00:00:00|     Apple| 359.56| 359.1665728068847|\n|2011-03-14 00:00:00|     Apple| 353.56| 353.7112783608036|\n|2011-03-23 00:00:00|     Apple| 339.19|338.82852635580537|\n|2011-04-04 00:00:00|     Apple| 341.19| 341.0931665879515|\n|2011-05-06 00:00:00|     Apple| 346.66|346.95394398147846|\n|2011-05-24 00:00:00|     Apple| 332.19| 332.4572073461659|\n|2011-05-31 00:00:00|     Apple| 347.83|347.15090411796666|\n+-------------------+----------+-------+------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Predictions saved to: gs://output_bucket_datasets/combined_predictions_df.csv\n"}], "source": "# Select relevant columns\npredictions_flat = predictions.select(\"Date\", \"Stock name\", \"Close\", \"prediction\")\n\n# Show predicted stock prices\npredictions_flat.show()\n\n# Save predictions to a CSV file in GCS bucket\noutput_path = \"gs://output_bucket_datasets/combined_predictions_df.csv\"\npredictions_flat.write.csv(output_path, header=True)\n\nprint(\"Predictions saved to:\", output_path)\n"}, {"cell_type": "code", "execution_count": null, "id": "a9c4be99-1c92-4761-b007-0311afb8ed32", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}